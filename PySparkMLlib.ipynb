{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLlib PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may know, PySpark is an open source programming framework for a quick data distributed processing. It was developed by Apache.\n",
    "\n",
    "Inside the framework you can find different tool, each of the with a particular purpose. In this case, we are going to use the library MLlib, which is a ML distributed library for training and testing regression, classification and clustering (and more) models. \n",
    "\n",
    "We will use diabetes dataset, where we have to predict if a particular person will have diabetes or not, depending on different blood and health indicators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing, as always, is importing different libraries we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE SESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing is creating a SparkSession, which is the connector (kind of an API) between our script and Spark Core. In this case we will call the app name *diabetesPrediction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = SparkSession.builder.appName(\"diabetesPrediction\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READ DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the session is created, is time to import data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = session.read.option(\"header\",\"True\").option(\"inferSchema\",\"True\").option(\"sep\",\",\").csv(\"./dataPySpark/diabetes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also count different rows and columns in our dataset, and also show the first rows. The most interesting thing is that you can see this different outputs by using same functions as pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay  768 rows 9 columnas en los datos\n"
     ]
    }
   ],
   "source": [
    "print(\"Hay \",df.count(),\"rows\",len(df.columns),\"columnas en los datos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|DiabetesPedigreeFunction|Age|Outcome|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "|          6|    148|           72|           35|      0|33.6|                   0.627| 50|      1|\n",
      "|          1|     85|           66|           29|      0|26.6|     0.35100000000000003| 31|      0|\n",
      "|          8|    183|           64|            0|      0|23.3|                   0.672| 32|      1|\n",
      "|          1|     89|           66|           23|     94|28.1|     0.16699999999999998| 21|      0|\n",
      "|          0|    137|           40|           35|    168|43.1|      2.2880000000000003| 33|      1|\n",
      "|          5|    116|           74|            0|      0|25.6|                   0.201| 30|      0|\n",
      "|          3|     78|           50|           32|     88|31.0|                   0.248| 26|      1|\n",
      "|         10|    115|            0|            0|      0|35.3|                   0.134| 29|      0|\n",
      "|          2|    197|           70|           45|    543|30.5|                   0.158| 53|      1|\n",
      "|          8|    125|           96|            0|      0| 0.0|     0.23199999999999998| 54|      1|\n",
      "|          4|    110|           92|            0|      0|37.6|                   0.191| 30|      0|\n",
      "|         10|    168|           74|            0|      0|38.0|                   0.537| 34|      1|\n",
      "|         10|    139|           80|            0|      0|27.1|      1.4409999999999998| 57|      0|\n",
      "|          1|    189|           60|           23|    846|30.1|     0.39799999999999996| 59|      1|\n",
      "|          5|    166|           72|           19|    175|25.8|      0.5870000000000001| 51|      1|\n",
      "|          7|    100|            0|            0|      0|30.0|                   0.484| 32|      1|\n",
      "|          0|    118|           84|           47|    230|45.8|                   0.551| 31|      1|\n",
      "|          7|    107|           74|            0|      0|29.6|                   0.254| 31|      1|\n",
      "|          1|    103|           30|           38|     83|43.3|                   0.183| 33|      0|\n",
      "|          1|    115|           70|           30|     96|34.6|                   0.529| 32|      1|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we can see the schema (variable types) of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Pregnancies: integer (nullable = true)\n",
      " |-- Glucose: integer (nullable = true)\n",
      " |-- BloodPressure: integer (nullable = true)\n",
      " |-- SkinThickness: integer (nullable = true)\n",
      " |-- Insulin: integer (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- DiabetesPedigreeFunction: double (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Outcome: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, time to remane columns and creating a list with the columns we will use in the future. Is important to rename the target column because, by default, pyspark takes the label column as a target one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"Outcome\",\"label\")\n",
    "columnas = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIPELINE CREATION FOR DATA TRANSFORMATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once renamed we will start by creating our pipeline. First stape will be adding an assembler in order to create one column for each row that will contain a vector with all row information. The result of this transformation will be the vectorized_features column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = columnas,outputCol = \"vectorized_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will create a scaler function for transforming all data to a normalized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(inputCol = \"vectorized_features\",outputCol = \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be pipeline result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "stages += [assembler]\n",
    "stages += [scaler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's transform data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the result after the transformations is a features column that contains a vector for each row and a label column for the supervised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[0.74371859296482...|    1|\n",
      "|[0.42713567839195...|    0|\n",
      "|[0.91959798994974...|    1|\n",
      "|[0.44723618090452...|    0|\n",
      "|[0.68844221105527...|    1|\n",
      "|[0.58291457286432...|    0|\n",
      "|[0.39195979899497...|    1|\n",
      "|[0.57788944723618...|    0|\n",
      "|[0.98994974874371...|    1|\n",
      "|[0.62814070351758...|    1|\n",
      "|[0.55276381909547...|    0|\n",
      "|[0.84422110552763...|    1|\n",
      "|[0.69849246231155...|    0|\n",
      "|[0.94974874371859...|    1|\n",
      "|[0.83417085427135...|    1|\n",
      "|[0.50251256281407...|    1|\n",
      "|[0.59296482412060...|    1|\n",
      "|[0.53768844221105...|    1|\n",
      "|[0.51758793969849...|    0|\n",
      "|[0.57788944723618...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select([\"features\",\"label\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN-TEST SPLIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this moment we will split the dataset in two groups, a train and a test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.8,0.2],seed = 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL EVALUATION WITH AUC DEFINITION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is time to define different models. We will try by using an easy logistic regression and a decission tree classifier and AUC metric for measuring our model quality. PySpark has the BinaryClassificationEvaluator class for AUC calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGISTIC REGRESSION MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's start with logisticRegression, that we are defining with the input data (features column), output data (labels) and max iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol = \"features\", labelCol = \"label\", maxIter = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training model over train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is time for training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, predict the results both in our training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction over train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictionsTrain = lrModel.transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction over test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictionsTest = lrModel.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regresion evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC 0.8357641867875993\n"
     ]
    }
   ],
   "source": [
    "print('Training Area Under ROC', evaluator.evaluate(lr_predictionsTrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC 0.8190164954870834\n"
     ]
    }
   ],
   "source": [
    "print('Test Area Under ROC', evaluator.evaluate(lr_predictionsTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result you can see a 0.83 AUC for the training set and a 0.82 AUC for the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECISION TREE CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the prediction process but now using a DecissionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we define the input (featuresCol) and output (labelCol) for training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training model over train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtModel = dt.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And last step, predict over the training and test data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction over train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_predictionTrain = dtModel.transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction over test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_predictionTest = dtModel.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC 0.7174592474567961\n"
     ]
    }
   ],
   "source": [
    "print('Training Area Under ROC', evaluator.evaluate(dt_predictionTrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC 0.6316526610644257\n"
     ]
    }
   ],
   "source": [
    "print('Test Area Under ROC', evaluator.evaluate(dt_predictionTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result you will see a 0.71 AUC for training set and 0.63 AUC for test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like in this case a LogisticRegression fits better than a DecissionTreeClassifier model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
